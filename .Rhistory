if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", case_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", case_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Case = case_name,
Item_Type = "Case"
) %>%
select(Case, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", case_name, "with", nrow(df), "data points"))
return(df)
}
# Updated function to scrape skin data with random delay
scrape_skin_chart_data <- function(skin_name, remDr, min_wait = 10, max_wait = 30) {
encoded_skin <- URLencode(skin_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_skin)
message(paste("Scraping skin:", skin_name))
remDr$navigate(url)
# Apply random delay
random_delay(min_wait, max_wait)
html <- remDr$getPageSource()[[1]]
html_clean <- gsub("[\r\n\t]", "", html)
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", skin_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", skin_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Skin = skin_name,
# Extract the case name from which this skin belongs
Case = names(case_skins)[sapply(case_skins, function(x) skin_name %in% x)],
Item_Type = "Skin"
) %>%
select(Case, Skin, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", skin_name, "with", nrow(df), "data points"))
return(df)
}
# Updated main scraping code with longer delays between items
# Flatten the case_skins list to get all skin names
all_skins <- unlist(case_skins)
# Start the RSelenium server
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)
remDr <- rD$client
# 1. First scrape all cases
message("Starting case scraping...")
all_cases_data <- list()
for (case_name in cases) {
case_data <- scrape_case_chart_data(case_name, remDr)
if (!is.null(case_data)) {
all_cases_data[[length(all_cases_data) + 1]] <- case_data
}
# Add extra delay between items
random_delay(15, 40)
}
all_cases_data <- bind_rows(all_cases_data)
message(paste("Completed scraping cases"))
# 2. Then scrape all skins
message("Starting skin scraping...")
all_skins_data <- list()
for (skin_name in all_skins) {
skin_data <- scrape_skin_chart_data(skin_name, remDr)
if (!is.null(skin_data)) {
all_skins_data[[length(all_skins_data) + 1]] <- skin_data
}
# Add extra delay between items
random_delay(15, 40)
}
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
cases <- c(
"Chroma Case", "Chroma 2 Case", "Gamma Case","Glove Case","Dreams & Nightmares Case"
)
case_skins <- list(
"Chroma Case" = c(
"Glock-18 | Catacombs (Minimal Wear)",           # pistol
"AK-47 | Cartel (Minimal Wear)",                 # rifle
"AWP | Man-o'-war (Minimal Wear)"                # AWP
),
"Chroma 2 Case" = c(
"USP-S | Torque (Minimal Wear)",                 # pistol
"M4A1-S | Hyper Beast (Minimal Wear)",           # rifle
"AWP | Worm God (Minimal Wear)"                  # AWP
),
"Gamma Case" = c(
"Glock-18 | Wasteland Rebel (Minimal Wear)",     # pistol
"M4A4 | Desolate Space (Minimal Wear)",          # rifle
"AWP | Phobos (Minimal Wear)"                    # AWP-preferred random
),
"Glove Case" = c(
"USP-S | Cyrex (Minimal Wear)",                  # pistol
"M4A4 | Buzz Kill (Minimal Wear)",               # rifle
"P90 | Shallow Grave (Minimal Wear)"             # random
),
"Dreams & Nightmares Case" = c(
"USP-S | Ticket to Hell (Minimal Wear)",         # pistol
"AK-47 | Nightwish (Minimal Wear)",              # rifle
"MP9 | Starlight Protector (Minimal Wear)"       # random
)
# Optinal Additional Cases
casesfinal <- c(
"Chroma Case", "Chroma 2 Case", "Falchion Case", "Shadow Case",
"Revolver Case", "Operation Wildfire Case", "Chroma 3 Case",
"Gamma Case", "Gamma 2 Case", "Glove Case", "Spectrum Case",
"Operation Hydra Case", "Spectrum 2 Case", "Clutch Case",
"Horizon Case", "Danger Zone Case", "Prisma Case", "CS20 Case",
"Shattered Web Case", "Prisma 2 Case", "Fracture Case",
"Operation Broken Fang Case", "Snakebite Case",
"Operation Riptide Case", "Dreams & Nightmares Case",
"Recoil Case", "Revolution Case", "Kilowatt Case", "Gallery Case"
)
# Add a random delay function
random_delay <- function(min_seconds = 10, max_seconds = 30) {
delay_time <- runif(1, min_seconds, max_seconds)
message(paste("Waiting for", round(delay_time, 1), "seconds..."))
Sys.sleep(delay_time)
}
# Updated function to scrape case data with random delay
scrape_case_chart_data <- function(case_name, remDr, min_wait = 10, max_wait = 30) {
encoded_case <- URLencode(case_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_case)
message(paste("Scraping case:", case_name))
remDr$navigate(url)
# Apply random delay
random_delay(min_wait, max_wait)
html <- remDr$getPageSource()[[1]]
html_clean <- gsub("[\r\n\t]", "", html)
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", case_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", case_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Case = case_name,
Item_Type = "Case"
) %>%
select(Case, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", case_name, "with", nrow(df), "data points"))
return(df)
}
# Updated function to scrape skin data with random delay
scrape_skin_chart_data <- function(skin_name, remDr, min_wait = 10, max_wait = 30) {
encoded_skin <- URLencode(skin_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_skin)
message(paste("Scraping skin:", skin_name))
remDr$navigate(url)
# Apply random delay
random_delay(min_wait, max_wait)
html <- remDr$getPageSource()[[1]]
html_clean <- gsub("[\r\n\t]", "", html)
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", skin_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", skin_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Skin = skin_name,
# Extract the case name from which this skin belongs
Case = names(case_skins)[sapply(case_skins, function(x) skin_name %in% x)],
Item_Type = "Skin"
) %>%
select(Case, Skin, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", skin_name, "with", nrow(df), "data points"))
return(df)
}
# Updated main scraping code with longer delays between items
# Flatten the case_skins list to get all skin names
all_skins <- unlist(case_skins)
# Start the RSelenium server
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)
remDr <- rD$client
# 1. First scrape all cases
message("Starting case scraping...")
all_cases_data <- list()
for (case_name in cases) {
case_data <- scrape_case_chart_data(case_name, remDr)
if (!is.null(case_data)) {
all_cases_data[[length(all_cases_data) + 1]] <- case_data
}
# Add extra delay between items
random_delay(15, 40)
}
all_cases_data <- bind_rows(all_cases_data)
message(paste("Completed scraping cases"))
# 2. Then scrape all skins
message("Starting skin scraping...")
all_skins_data <- list()
for (skin_name in all_skins) {
skin_data <- scrape_skin_chart_data(skin_name, remDr)
if (!is.null(skin_data)) {
all_skins_data[[length(all_skins_data) + 1]] <- skin_data
}
# Add extra delay between items
random_delay(15, 40)
}
all_skins_data <- bind_rows(all_skins_data)
message(paste("Completed scraping skins"))
# Stop RSelenium
remDr$close()
rD$server$stop()
# Combine the case and skin data
# Note: We need to modify the skin data to match the structure before combining
all_skins_data_for_combine <- all_skins_data %>%
rename(Item = Skin) %>%
select(Case, Item, Item_Type, date, price_usd, volume)
all_cases_data_for_combine <- all_cases_data %>%
mutate(Item = Case) %>%
select(Case, Item, Item_Type, date, price_usd, volume)
# Combine both datasets
all_items_data <- bind_rows(all_cases_data_for_combine, all_skins_data_for_combine)
# Data Refining for all items
all_items_data_clean <- all_items_data %>%
mutate(date = as.POSIXct(date, tz = "UTC"))
# Daily history for all items
daily_item_closes <- all_items_data_clean %>%
mutate(
hour = hour(date),
day = as.Date(date)
) %>%
filter(hour == 1) %>%  # Keep only rows with time == 1:00
group_by(Case, Item, Item_Type, day) %>%  # In case multiple entries at 1:00
slice_max(order_by = date, n = 1) %>%
ungroup() %>%
select(Case, Item, Item_Type, date, price_usd, volume) %>%
arrange(Case, Item, date)
# Past month data for all items
past_month_items <- all_items_data_clean %>%
filter(date >= Sys.Date() - months(1)) %>%
group_by(Case, Item, Item_Type) %>%
arrange(date) %>%
slice(-1) %>%  # removes the first row of each group
ungroup()
#Exchange Rates TidyQuant Ticker: "CAD=X"
fx_usdcad <- tq_get("CAD=X", get = "stock.prices", from = "2010-01-01")
fx_usdcad_clean <- fx_usdcad %>%
select(date, fx_rate = close)
fx_usdcad_filled <- fx_usdcad_clean %>%
complete(date = seq.Date(min(date), max(date), by = "day")) %>%
fill(fx_rate, .direction = "down")
# Convert to CAD
CAD_Converted_Items <- all_items_data_clean %>%
mutate(date_only = as.Date(date)) %>%
left_join(fx_usdcad_filled, by = c("date_only" = "date")) %>%
mutate(price_cad = price_usd * fx_rate) %>%
drop_na()
CAD_Selected_Items <- CAD_Converted_Items %>%
select(Case, Item, Item_Type, date, price_cad, volume)
# Daily history (CAD) for all items
Daily_CAD_Items <- CAD_Selected_Items %>%
mutate(
hour = hour(date),
day = as.Date(date)
) %>%
filter(hour == 1) %>%  # Keep only rows with time == 1:00
group_by(Case, Item, Item_Type, day) %>%  # In case multiple entries at 1:00
slice_max(order_by = date, n = 1) %>%
ungroup() %>%
select(Case, Item, Item_Type, date, price_cad, volume) %>%
arrange(Case, Item, date)
# Past month data (CAD) for all items
Month_CAD_Items <- CAD_Selected_Items %>%
filter(date >= Sys.Date() - months(1)) %>%
group_by(Case, Item, Item_Type) %>%
arrange(date) %>%
slice(-1) %>%  # removes the first row of each group
ungroup()
# Example visualization code - comparing cases and their skins
# For each case in case_skins, plot the case price and its skin prices
plot_case_and_skins <- function(case_name) {
case_data <- Month_CAD_Items %>%
filter(Case == case_name)
ggplot(case_data, aes(x = date, y = price_cad, color = Item, linetype = Item_Type)) +
geom_line() +
labs(title = paste("Price Comparison:", case_name, "and its Skins (CAD)"),
x = "Date",
y = "Price (CAD)") +
theme_minimal() +
theme(legend.position = "right")
}
# Save separate dataframes for cases and skins
cases_data <- all_items_data_clean %>% filter(Item_Type == "Case")
skins_data <- all_items_data_clean %>% filter(Item_Type == "Skin")
# Write a function to run the plots for each case in case_skins
case_plots <- lapply(names(case_skins), plot_case_and_skins)
# Save the raw scraped data
readr::write_csv(all_cases_data, "steam_cases_raw.csv")
readr::write_csv(all_skins_data, "steam_skins_raw.csv")
View(all_items_data)
# Save the raw scraped data
readr::write_csv(all_cases_data, "steam_cases_raw.csv")
readr::write_csv(all_skins_data, "steam_skins_raw.csv")
readr::write_csv(all_items_data_clean, "steam_items_clean.csv")
readr::write_csv(CAD_Selected_Items, "steam_items_cad.csv")
readr::write_csv(Daily_CAD_Items, "steam_items_daily_cad.csv")
readr::write_csv(Month_CAD_Items, "steam_items_month_cad.csv")
View(all_items_data_clean)
View(all_items_data)
View(all_items_data_clean)
View(CAD_Converted_Items)
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
library(readr)
url_cases <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_cases_raw.csv?token=GHSAT0AAAAAAC76LQ4NSZQFOBUVWQMOEEVMZ7UNCEA"
steam_cases_raw <- read_csv(url_cases)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
url_cases <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_cases_raw.csv?token=GHSAT0AAAAAAC76LQ4N7GRPMWMTF5Q6EANYZ7UNJNA"
steam_cases_raw <- read_csv(url_cases)
View(steam_skins_raw)
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
library(readr)
cases <- c(
"Chroma Case", "Chroma 2 Case", "Gamma Case","Glove Case","Dreams & Nightmares Case"
)
case_skins <- list(
"Chroma Case" = c(
"Glock-18 | Catacombs (Minimal Wear)",           # pistol
"AK-47 | Cartel (Minimal Wear)",                 # rifle
"AWP | Man-o'-war (Minimal Wear)"                # AWP
),
"Chroma 2 Case" = c(
"USP-S | Torque (Minimal Wear)",                 # pistol
"M4A1-S | Hyper Beast (Minimal Wear)",           # rifle
"AWP | Worm God (Minimal Wear)"                  # AWP
),
"Gamma Case" = c(
"Glock-18 | Wasteland Rebel (Minimal Wear)",     # pistol
"M4A4 | Desolate Space (Minimal Wear)",          # rifle
"AWP | Phobos (Minimal Wear)"                    # AWP-preferred random
),
"Glove Case" = c(
"USP-S | Cyrex (Minimal Wear)",                  # pistol
"M4A4 | Buzz Kill (Minimal Wear)",               # rifle
"P90 | Shallow Grave (Minimal Wear)"             # random
),
"Dreams & Nightmares Case" = c(
"USP-S | Ticket to Hell (Minimal Wear)",         # pistol
"AK-47 | Nightwish (Minimal Wear)",              # rifle
"MP9 | Starlight Protector (Minimal Wear)"       # random
)
# Optinal Additional Cases
casesfinal <- c(
"Chroma Case", "Chroma 2 Case", "Falchion Case", "Shadow Case",
"Revolver Case", "Operation Wildfire Case", "Chroma 3 Case",
"Gamma Case", "Gamma 2 Case", "Glove Case", "Spectrum Case",
"Operation Hydra Case", "Spectrum 2 Case", "Clutch Case",
"Horizon Case", "Danger Zone Case", "Prisma Case", "CS20 Case",
"Shattered Web Case", "Prisma 2 Case", "Fracture Case",
"Operation Broken Fang Case", "Snakebite Case",
"Operation Riptide Case", "Dreams & Nightmares Case",
"Recoil Case", "Revolution Case", "Kilowatt Case", "Gallery Case"
)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
url_cases <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_cases_raw.csv?token=GHSAT0AAAAAAC76LQ4N7GRPMWMTF5Q6EANYZ7UNJNA"
steam_cases_raw <- read_csv(url_cases)
# Combine the case and skin data
# Note: We need to modify the skin data to match the structure before combining
all_skins_data_for_combine <- all_skins_data %>%
rename(Item = Skin) %>%
select(Case, Item, Item_Type, date, price_usd, volume)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
all_skins_data <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
all_skins_data <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(readr)
url_skins <- "https://raw.githubusercontent.com/AndrewPettifor25/FIN_450_Steam_Final/refs/heads/main/steam_skins_raw.csv?token=GHSAT0AAAAAAC76LQ4MJRCAELCMZQHGZUFKZ7UNG6A"
steam_skins_raw <- read_csv(url_skins)
library(shiny); runApp('~/GitHub/FIN_450_Steam_Final/CS2_App_Refined.R')
