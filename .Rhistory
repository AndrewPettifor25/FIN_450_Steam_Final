matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", case_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", case_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Case = case_name
) %>%
select(Case, date, price_usd, volume) %>%
filter(!is.na(date))
return(df)
}
# Start
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)
remDr <- rD$client
# Scrape
all_data <- purrr::map_dfr(cases, ~ scrape_case_chart_data(.x, remDr))
# Stop
remDr$close()
rD$server$stop()
all_data_clean <- all_data %>%
mutate(date = as.POSIXct(date, tz = "UTC"))
daily_closes <- all_data_clean %>%
mutate(
hour = hour(date),
day = as.Date(date)
) %>%
filter(hour == 1) %>%  # Keep only rows with time == 1:00
group_by(Case, day) %>%  # In case multiple entries at 1:00, just in case
slice_max(order_by = date, n = 1) %>%
ungroup() %>%
select(Case, date, price_usd, volume) %>%
arrange(Case, date)
past_month <- all_data_clean %>%
filter(date >= Sys.Date() - months(1)) %>%
group_by(Case) %>%
arrange(date) %>%
slice(-1) %>%  # removes the first row of each group
ungroup()
fx_usdcad <- tq_get("CAD=X", get = "stock.prices", from = "2010-01-01")
fx_usdcad_clean <- fx_usdcad %>%
select(date, fx_rate = close)
fx_usdcad_filled <- fx_usdcad_clean %>%
complete(date = seq.Date(min(date), max(date), by = "day")) %>%
fill(fx_rate, .direction = "down")
CAD_Converted <- all_data_clean %>%
mutate(date_only = as.Date(date)) %>%
left_join(fx_usdcad_filled, by = c("date_only" = "date")) %>%
mutate(price_cad = price_usd * fx_rate) %>%
drop_na()
CAD_Selected <- CAD_Converted %>%
select(Case, date, price_cad, volume)
Daily_CAD <- CAD_Selected %>%
mutate(
hour = hour(date),
day = as.Date(date)
) %>%
filter(hour == 1) %>%  # Keep only rows with time == 1:00
group_by(Case, day) %>%  # In case multiple entries at 1:00, just in case
slice_max(order_by = date, n = 1) %>%
ungroup() %>%
select(Case, date, price_cad, volume) %>%
arrange(Case, date)
Month_CAD <- CAD_Selected %>%
filter(date >= Sys.Date() - months(1)) %>%
group_by(Case) %>%
arrange(date) %>%
slice(-1) %>%  # removes the first row of each group
ungroup()
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
case_skins <- list(
"Chroma Case" = c(
"AK-47 | Cartel (Minimal Wear)",
"M4A4 | Dragon King (Minimal Wear)",
"AWP | Man-o'-war (Minimal Wear)"
),
"Chroma 2 Case" = c(
"Five-SeveN | Monkey Business (Minimal Wear)",
"AWP | Worm God (Minimal Wear)",
"Galil AR | Eco (Minimal Wear)"
),
"Gamma Case" = c(
"M4A4 | Desolate Space (Minimal Wear)",
"Glock-18 | Wasteland Rebel (Minimal Wear)",
"AUG | Aristocrat (Minimal Wear)"
),
"Glove Case" = c(
"MP7 | Cirrus (Minimal Wear)",
"P2000 | Turf (Minimal Wear)",
"M4A1-S | Flashback (Minimal Wear)"
),
"Dreams & Nightmares Case" = c(
"MP5-SD | Necro Jr. (Minimal Wear)",
"AK-47 | Nightwish (Minimal Wear)",
"USP-S | Ticket to Hell (Minimal Wear)"
),
"Recoil Case" = c(
"FAMAS | Meow 36 (Minimal Wear)",
"AK-47 | Ice Coaled (Minimal Wear)",
"AWP | Chromatic Aberration (Minimal Wear)"
),
"Prisma 2 Case" = c(
"AK-47 | Phantom Disruptor (Minimal Wear)",
"Desert Eagle | Blue Ply (Minimal Wear)",
"M4A1-S | Player Two (Minimal Wear)"
)
all_skins <- unlist(case_skins)
crape_skin_chart_data <- function(skin_name, remDr, wait_time = 6) {
encoded_skin <- URLencode(skin_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_skin)
remDr$navigate(url)
Sys.sleep(wait_time)
html <- remDr$getPageSource()[[1]]
html_clean <- gsub("[\r\n\t]", "", html)
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", skin_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", skin_name, ":", e$message))
return(NULL)
})
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Skin = skin_name,
# Extract the case name from which this skin belongs
Case = names(case_skins)[sapply(case_skins, function(x) skin_name %in% x)]
) %>%
select(Case, Skin, date, price_usd, volume) %>%
filter(!is.na(date))
return(df)
}
# Start
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)
remDr <- rD$client
# Scrape all skins
all_skins_data <- purrr::map_dfr(all_skins, ~ scrape_skin_chart_data(.x, remDr))
library(RSelenium)
library(jsonlite)
library(stringr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(tidyquant)
# Define cases and case skins (your original data)
cases <- c(
"Chroma Case", "Chroma 2 Case", "Gamma Case","Glove Case","Dreams & Nightmares Case", "Recoil Case","Prisma 2 Case"
)
case_skins <- list(
"Chroma Case" = c(
"AK-47 | Cartel (Minimal Wear)",
"M4A4 | Dragon King (Minimal Wear)",
"AWP | Man-o'-war (Minimal Wear)"
),
"Chroma 2 Case" = c(
"Five-SeveN | Monkey Business (Minimal Wear)",
"AWP | Worm God (Minimal Wear)",
"Galil AR | Eco (Minimal Wear)"
),
"Gamma Case" = c(
"M4A4 | Desolate Space (Minimal Wear)",
"Glock-18 | Wasteland Rebel (Minimal Wear)",
"AUG | Aristocrat (Minimal Wear)"
),
"Glove Case" = c(
"MP7 | Cirrus (Minimal Wear)",
"P2000 | Turf (Minimal Wear)",
"M4A1-S | Flashback (Minimal Wear)"
),
"Dreams & Nightmares Case" = c(
"MP5-SD | Necro Jr. (Minimal Wear)",
"AK-47 | Nightwish (Minimal Wear)",
"USP-S | Ticket to Hell (Minimal Wear)"
),
"Recoil Case" = c(
"FAMAS | Meow 36 (Minimal Wear)",
"AK-47 | Ice Coaled (Minimal Wear)",
"AWP | Chromatic Aberration (Minimal Wear)"
),
"Prisma 2 Case" = c(
"AK-47 | Phantom Disruptor (Minimal Wear)",
"Desert Eagle | Blue Ply (Minimal Wear)",
"M4A1-S | Player Two (Minimal Wear)"
)
)
casesfinal <- c(
"Chroma Case", "Chroma 2 Case", "Falchion Case", "Shadow Case",
"Revolver Case", "Operation Wildfire Case", "Chroma 3 Case",
"Gamma Case", "Gamma 2 Case", "Glove Case", "Spectrum Case",
"Operation Hydra Case", "Spectrum 2 Case", "Clutch Case",
"Horizon Case", "Danger Zone Case", "Prisma Case", "CS20 Case",
"Shattered Web Case", "Prisma 2 Case", "Fracture Case",
"Operation Broken Fang Case", "Snakebite Case",
"Operation Riptide Case", "Dreams & Nightmares Case",
"Recoil Case", "Revolution Case", "Kilowatt Case", "Gallery Case"
)
# Helper function for randomized delays
random_delay <- function(base_seconds = 6, variation = 3) {
delay_time <- base_seconds + runif(1, -variation, variation)
delay_time <- max(delay_time, 3)  # Ensure minimum 3 second delay
message(paste("Waiting for", round(delay_time, 1), "seconds..."))
Sys.sleep(delay_time)
}
# Function to retry operations with exponential backoff
retry_with_backoff <- function(expr, max_retries = 5, initial_backoff = 10) {
retries <- 0
while (retries <= max_retries) {
result <- try(expr, silent = TRUE)
if (!inherits(result, "try-error")) {
return(result)
}
retries <- retries + 1
if (retries > max_retries) {
stop("Maximum retry attempts reached")
}
# Calculate backoff time with exponential increase and jitter
backoff_time <- initial_backoff * (2^(retries - 1)) * (1 + runif(1, -0.1, 0.1))
message(paste("Attempt", retries, "failed. Retrying in", round(backoff_time, 1), "seconds..."))
Sys.sleep(backoff_time)
}
}
# Function to check if we need to refresh the browser session
check_and_refresh_session <- function(remDr, session_age, max_session_age = 20) {
if (session_age >= max_session_age) {
message("Refreshing browser session...")
try(remDr$close(), silent = TRUE)
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)
remDr <- rD$client
# Set a random user agent
user_agents <- c(
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
)
# Setting user agent requires extra steps in Selenium
# This is simplified - in practice you might need additional configuration
random_ua <- sample(user_agents, 1)
return(list(driver = remDr, session_age = 0))
}
return(list(driver = remDr, session_age = session_age + 1))
}
# Function to save progress periodically
save_progress <- function(data, filename, item_index, total_items) {
saveRDS(data, filename)
message(paste("Progress saved:", item_index, "of", total_items, "items processed"))
}
# Improved function to scrape case data with retry and session management
scrape_case_chart_data <- function(case_name, remDr, wait_base = 6, wait_var = 3) {
encoded_case <- URLencode(case_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_case)
message(paste("Scraping case:", case_name))
# Navigate with retry logic
navigate_result <- retry_with_backoff({
remDr$navigate(url)
random_delay(wait_base, wait_var)
TRUE
}, max_retries = 3)
if (!navigate_result) {
message(paste("❌ Failed to navigate to URL for:", case_name))
return(NULL)
}
# Get page source with retry logic
html_result <- retry_with_backoff({
remDr$getPageSource()[[1]]
}, max_retries = 3)
if (is.null(html_result)) {
message(paste("❌ Failed to get page source for:", case_name))
return(NULL)
}
html_clean <- gsub("[\r\n\t]", "", html_result)
# Extract data pattern
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", case_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", case_name, ":", e$message))
return(NULL)
})
if (is.null(chart_data) || nrow(chart_data) == 0) {
message(paste("❌ No chart data found for:", case_name))
return(NULL)
}
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Case = case_name,
Item_Type = "Case"
) %>%
select(Case, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", case_name, "with", nrow(df), "data points"))
return(df)
}
# Improved function to scrape skin data with retry and session management
scrape_skin_chart_data <- function(skin_name, remDr, wait_base = 6, wait_var = 3) {
encoded_skin <- URLencode(skin_name)
url <- paste0("https://steamcommunity.com/market/listings/730/", encoded_skin)
message(paste("Scraping skin:", skin_name))
# Navigate with retry logic
navigate_result <- retry_with_backoff({
remDr$navigate(url)
random_delay(wait_base, wait_var)
TRUE
}, max_retries = 3)
if (!navigate_result) {
message(paste("❌ Failed to navigate to URL for:", skin_name))
return(NULL)
}
# Get page source with retry logic
html_result <- retry_with_backoff({
remDr$getPageSource()[[1]]
}, max_retries = 3)
if (is.null(html_result)) {
message(paste("❌ Failed to get page source for:", skin_name))
return(NULL)
}
html_clean <- gsub("[\r\n\t]", "", html_result)
# Extract data pattern
pattern <- "var line1=\\[(\\[.*?\\])\\];"
matches <- stringr::str_match(html_clean, pattern)
if (is.na(matches[2])) {
message(paste("❌ Could not extract line1 for:", skin_name))
return(NULL)
}
json_array <- paste0("[", matches[2], "]")
chart_data <- tryCatch({
fromJSON(json_array)
}, error = function(e) {
message(paste("❌ JSON parse error for", skin_name, ":", e$message))
return(NULL)
})
if (is.null(chart_data) || nrow(chart_data) == 0) {
message(paste("❌ No chart data found for:", skin_name))
return(NULL)
}
num_cols <- ncol(chart_data)
df <- tibble(
time = chart_data[, 1],
price_usd = as.numeric(chart_data[, 2]),
volume = if (num_cols >= 3) as.numeric(chart_data[, 3]) else NA_real_
) %>%
mutate(
time_clean = str_remove(time, " \\+0$"),
date = parse_date_time(time_clean, orders = "b d Y H", tz = "UTC"),
Skin = skin_name,
# Extract the case name from which this skin belongs
Case = names(case_skins)[sapply(case_skins, function(x) skin_name %in% x)],
Item_Type = "Skin"
) %>%
select(Case, Skin, Item_Type, date, price_usd, volume) %>%
filter(!is.na(date))
message(paste("✓ Successfully scraped data for:", skin_name, "with", nrow(df), "data points"))
return(df)
}
# Scrape data with rate limiting, session management, and checkpoint saving
scrape_with_rate_limiting <- function() {
# Flatten the case_skins list to get all skin names
all_skins <- unlist(case_skins)
# Create output directory for checkpoint files
dir.create("steam_scrape_checkpoints", showWarnings = FALSE)
# Try to load previous checkpoint if it exists
checkpoint_file <- "steam_scrape_checkpoints/scrape_checkpoint.rds"
if (file.exists(checkpoint_file)) {
checkpoint <- readRDS(checkpoint_file)
message("Resuming from checkpoint")
all_cases_data <- checkpoint$cases_data
all_skins_data <- checkpoint$skins_data
completed_cases <- checkpoint$completed_cases
completed_skins <- checkpoint$completed_skins
} else {
all_cases_data <- tibble()
all_skins_data <- tibble()
completed_cases <- character(0)
completed_skins <- character(0)
}
# Start the RSelenium server with custom profile settings
message("Starting RSelenium server...")
# Adding custom Firefox profile to make it look more like a real browser
fprof <- makeFirefoxProfile(list(
browser.sessionstore.interval = 60000,
browser.cache.disk.enable = TRUE,
browser.cache.memory.enable = TRUE,
privacy.trackingprotection.enabled = FALSE,
dom.webnotifications.enabled = FALSE,
media.volume_scale = 0,
media.autoplay.default = 0
))
rD <- rsDriver(browser = "firefox", port = 4444L, extraCapabilities = fprof, verbose = FALSE)
remDr <- rD$client
# Set window size to look more like a real browser
remDr$setWindowSize(1366, 768)
# Set a user agent
# (Note: This is simplified - a proper implementation might require more configuration)
# Main scraping logic
session_age <- 0
# 1. Scrape cases that haven't been completed yet
message("Starting case scraping...")
cases_to_scrape <- setdiff(cases, completed_cases)
for (i in seq_along(cases_to_scrape)) {
case_name <- cases_to_scrape[i]
# Check and refresh session if needed
session_result <- check_and_refresh_session(remDr, session_age)
remDr <- session_result$driver
session_age <- session_result$session_age
# Try to scrape the case data
case_data <- scrape_case_chart_data(case_name, remDr)
if (!is.null(case_data)) {
all_cases_data <- bind_rows(all_cases_data, case_data)
completed_cases <- c(completed_cases, case_name)
# Save checkpoint after each successful scrape
saveRDS(
list(
cases_data = all_cases_data,
skins_data = all_skins_data,
completed_cases = completed_cases,
completed_skins = completed_skins
),
checkpoint_file
)
message(paste("Progress:", i, "of", length(cases_to_scrape), "cases processed"))
}
# Add a longer delay between items
random_delay(10, 5)
}
message(paste("Completed scraping", length(completed_cases), "of", length(cases), "cases"))
# 2. Scrape skins that haven't been completed yet
message("Starting skin scraping...")
skins_to_scrape <- setdiff(all_skins, completed_skins)
for (i in seq_along(skins_to_scrape)) {
skin_name <- skins_to_scrape[i]
# Check and refresh session if needed
session_result <- check_and_refresh_session(remDr, session_age)
remDr <- session_result$driver
session_age <- session_result$session_age
# Try to scrape the skin data
skin_data <- scrape_skin_chart_data(skin_name, remDr)
if (!is.null(skin_data)) {
all_skins_data <- bind_rows(all_skins_data, skin_data)
completed_skins <- c(completed_skins, skin_name)
# Save checkpoint after each successful scrape
saveRDS(
list(
cases_data = all_cases_data,
skins_data = all_skins_data,
completed_cases = completed_cases,
completed_skins = completed_skins
),
checkpoint_file
)
message(paste("Progress:", i, "of", length(skins_to_scrape), "skins processed"))
}
# Add a longer delay between items
random_delay(10, 5)
}
message(paste("Completed scraping", length(completed_skins), "of", length(all_skins), "skins"))
# Stop RSelenium
remDr$close()
rD$server$stop()
return(list(cases_data = all_cases_data, skins_data = all_skins_data))
}
# Run the improved scraping function
scrape_results <- scrape_with_rate_limiting()
